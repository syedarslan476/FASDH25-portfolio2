# Folder Structure:
├── AI_documentation
            ├── AI_documentation_Abdus_Salam
            ├── AI_documentation_Atiqa_Rani
            ├── AI_documentation_syedali_arsalan
├── counts/:
    ├── ner_counts
    ├── regex_counts

├── gazetteers/:
       ├── countries
       ├── geonames_gaza_selection
       ├── README


├── maps/:
    ├── regex_map.html
    ├── regex_map.png
    ├── ner_map.html
    ├── ner_map.png


├── scripts/:
          ├── regex_script_final.py
          ├── Regex_Map_Script
          ├──ner_map_script
          ├── ner_gazetteer

├── Copy_of_Gaza_NER2_Abdus_Syed_Atiqa

├── README file

#Folder Structure Description:

/AI_Documentations/: We have a folder titled "AI Documentation", which contains three subfolders named after our team members: Abdus Salam, Atiqa Rani, and Arsalan.

/Counts/: The counts folder contains files that store data derived from different entity recognition methods. It includes ner_counts, which holds counts obtained through Named Entity Recognition (NER), and regex_counts, which contains counts generated using regular expression-based pattern matching. It has place names along with their months and counts. 

/Gazetters/: We have a folder named gazetteers, and inside it is several subfolders and a file. These include a subfolder called countries, another subfolder named geonames_gaza_selection, and a README file.

/Maps/: Inside the "maps/" folder, we have the following sub-files which are following:

regex_map.html

regex_map.png

ner_map.html

ner_map.png

/scripts/: This folder contains the python script (regex_script_final.py). The script reads the gazetteer file, builds regex patterns, and scans articles for place names. 
It also has the (Regex_Map-Script) which builds regex maps. It also has the (ner_map_Script) and also the (ner-gazetteer).


/Copy_of_Gaza_NER2_Abdus_Syed_Atiqa/: This have the script of the step 2B. it also contains the script of this folder(build_gazetteer.py). we have downloaded both the scripts togetherly. 

/README file/: This is file which have the final README file.




# Mini project 02: Visualizing Place Names in news Articles: 
Our project processes a collection of news articles to extract and visualize mentions of Gaza place names using two different methods. First, it uses regular expressions (regex-based script) which searches for places listed in a gazetteer and counts how often they appear each month. Second method it uses is named entity recognition (NER) with natural language processing tools and also geocoding. The results of this are then compared, mapped, and documented to explore how different methods effect what we find and how we interpret it.

## 2A: Extracting Gaza Place Names Using Regex and Gazetteer:
This is the first step of our project which involves building a dictionary of Gaza place names using a gazetteer file. It aims to identify and count place names from a set of articles related to Gaza. To do this we have used regular expressions (regex) in python. The place names are matched against a gazetteer file which contains standardized and alternate place names. This step ensures that both standard and alternate names are counted accurately across the articles.  This step is necessary to match place names accurately in the article texts even when those names are spelled differently or written in alternate forms. 

Key steps:

### Use the gazetteer and corpus in the portfolio repo:
### Improve the recall of place names:
1.	 Very firstly, we began with creating a copy of the regex script that we worked on during our class. We pasted this file into our new portfolio (FASDH25-portfolio2) and then started working on it. We change the name of file and saved it as “regex_script_atiqa_rani.py”.
2.	Next changed the folder and path of our script to the gazetteer file and corpus folder which were present in the “FASDH25-portfolio2”. (folder = “articles” and path = “gazetteers/geonames_gaza_selection.tsv"). This step ensured that we are working with the correct files/articles.
3.	We will now build a script to recognise place names from the files mentioned above. In this step, we will process the gazetteer file which contains the place names along with the alternate names. The gazetteer file (in .tsv format) is opened using “utf-8” encoding. This will ensure that all the characters, including special characters are read correctly.
4.	 In our class, we only focused on the main place name (from the asciiname column). However, this method missed many mentions of places due to spelling variations or different names mentioned in the article. Therefore, in this project, the script has been modified. To address this, the script has been updated to create a regular expression (regex) which includes both the main place name (from the first column) and all alternate names (from the sixth column of the gazetteer). All these names are combined into a single regex pattern using the OR operator (“|” symbol). This tells the script to match any of the listed variants. Use of this symbol improves the recall and makes sure that no name variant is missed during the text-scanning process.  
5.	Each of these patterns is then stored in a dictionary. In this dictionary we have the key in the main place name, and the value includes the full pattern and a counter. 
6.	Counter is simply a number which keeps the track of how many times something happens. In our script it is used to count how many times a place name or alternate is appeared in the articles. 
7.	The gazetteer file is formatted as a .tsv file, which helps organise structured data. Each line (or row) in the file corresponds to one place entry and is separated using the newline character \n. with each line, values such as the main place name, coordinates, and alternate names are separated by tab characters (\t). This structure allows the script to extract the correct columns, such as the main name (column 0) and alternate names (column 5), efficiently for further processing. 
8.	Before going to process each row, we have also ensured that there are at least six columns (index 5), which is the column which has the alternate names. This makes sure that if any row does not contain this column, then it will be skipped to avoid the error. (len(column) < 6)
9.	In case when the alternate names contain any special character like dots, to make sure that such names are not missed we have used “re.script()”. This makes the regex-safe and includes all names even if they have special characters. 
10.	After getting the dictionary of place names with their regex patterns, the script then scans through the articles in the “article” folder. The scripts work for each article using the regex patterns and searches for mentions of place names with their alternate names. This process will help us in tracking how many times each place is mentioned across the entire collection of articles. 
11.	At the end, this gives us a count of how many times each place is mentioned. Now using this, we will move to the next step. 

### Include only articles related to the current war: 
Now once we are done with the list of place names, here in this step we will filter our articles based on the war start date. As the filenames of our articles contain the dates in the format (YYYY-MM-DD_filename.txt), so we will now add a condition to our script that ensures it only extract data of articles written after the date 2023-10-07. We define the start date of war as (war_start_date).
For each article, the script looks at the filename. It will split the name at the underscore and takes only the first part as the date in the format (YYYY-MM-DD) and compare it with war start date. If any article was written before this date, then the script will not read it and skips it. 
For all other articles written after this date the script will read the content (using open (file_path, encoding=”utf-8”)) and search for place names using the previously built regex patterns. For each time a place name and its alternate name are found, then the script updates the count. Thus, this step will ensure that only recent war-time mentions are considered in the further analysis of data. 

### Count the mentions of each place name per month:
In this step we have to count how many times each place was mentioned during each month. For this we will make an additional dictionary “mentions_per_month”. This dictionary is structured in two levels. The first level has the names for places, and for each place there is second dictionary which stores the frequency of how many times the place is mentioned in each specific month.  
To do this step the script first checks whether a place is found in the article (if count > 0). If yes, then it checks whether that place already exists in the mentions_per_month dictionary. If it is not present, then the script adds it. After this it checks if month which is extracted from the filename already exists under that place or not. If it is not, the script creates a new entry for the month and starts the count. If it is yes, then it simply increases the count with the number of mentions in the article. 
finally, the scripts give the output, showing each place with the number of times it is mentioned in each month. The output is given in a list of rows (place, month, and count). 

## Write a tsv file that is called “regex_counts.tsv”:
Now after above all codes the script gives the output. It will save place name mentions by month into a TSV file called as (regex_counts.tsv). It will have columns for placename, month, and count. We will also do a sanity check for this.
At the last we will rename final script file by changing it from (regex_script_atiqa_rani.py) to (regex_script_final.py).



## 2B Place Name Extraction from January 2024 News Articles
This step analyzes a collection of news articles from January 2024 to extract and visualize place names mentioned in the text. Two techniques are used: Named Entity Recognition (NER) using the Stanza NLP library and a Regex method. The goal is to identify geographic references, clean and normalize them, c, and export the results for visualization. This project was completed as part of the FASDH25 Digital Humanities course. I got help from ChatGPT, class presentations, and colleagues.

Key Steps: 

### Installation and Setup:
The first step I took was setting up the environment to perform Named Entity Recognition using Stanza. I installed it using !pip install stanza and downloaded the English model with stanza.download("en"). After that, I imported all the required libraries such as stanza, os, re, and pandas, and then initialized an NLP pipeline using:
nlp = stanza.Pipeline(lang="en", processors='tokenize,mwt,ner')
This command sets up the language pipeline to tokenize the text, expand multi-word tokens, and recognize named entities automatically.
This setup was essential because Stanza provides high-quality pretrained models capable of identifying parts of text, especially geographic names. Without this NLP pipeline, I would have had to go through each article manually to identify locations, which would be time-consuming and prone to errors. Automating this process laid a solid foundation for the project.

### Filtering January 2024 Articles:
Once my environment was ready, I focused on narrowing down the dataset. I had a large collection of news articles from all of 2024. To ensure consistency and relevance, I chose to work only with January articles. I used Python's os module to list all the files and filtered the ones that started with 2024-01-:
jan_files = [file for file in files if file.startswith("2024-01-") and file.endswith(".txt")]
This gave me a precise subset of January articles from the full dataset.
This filtering was crucial because it allowed me to perform a time-bound analysis of geographic references in the media. By using the same subset of articles for both NER and Regex methods, I ensured fairness and consistency in my comparison. It also helped reduce the volume of data, making processing faster and more manageable.

### Named Entity Recognition (NER) with Stanza:
After filtering the dataset, I applied the Stanza NLP pipeline to each article to extract place names. For each file, I read the text and ran it through nlp(text). I then looped through the entities and selected those with types GPE (Geo-Political Entity) or LOC (Location):
for e in doc.entities:
    if e.type in ["GPE", "LOC"]:
        places[e.text] = places.get(e.text, 0) + 1
This allowed me to build a dictionary where keys were place names and values were their frequency in the text.
This step was powerful because it enabled automatic extraction of place names from natural language text. It eliminated the need for manually tagging data and gave me insight into how frequently different places were mentioned. The pretrained NER model was particularly helpful in identifying even complex or multi-word location names.

### Cleaning and Normalizing Place Names:
The place names extracted using NER were often inconsistent. Variations in spelling, abbreviations, or punctuation caused similar entities to appear as different entries. To solve this, I used regex to clean the text and a standardization dictionary to merge equivalent names:
standard_names = {
    'u.s.': 'United States',
    'britain': 'United Kingdom',
    'gaza city': 'Gaza',
    # ... more mappings
}
Each place name was processed to remove punctuation, possessives, and unnecessary prefixes, then checked against this dictionary.
This cleaning process was essential for improving accuracy. For example, mentions of "UK," "Britain," and "United Kingdom" were merged into one standardized label. Without this step, the frequency counts would be fragmented and misleading. Kulsoom helped me build the standardization dictionary, which greatly improved the quality of this part of the project.

### Storing Cleaned Data:
After cleaning and normalizing the place names, I saved the final output in a tab-separated values (TSV) file. This was done with the following code:
with open("ner_counts.tsv", mode="w", encoding="utf-8") as file:
    file.write("Place\tCount\n")
    for place, count in normalized_places.items():
        file.write(f"{place}\t{count}\n")
The .tsv format ensured each row had a place name and its count, separated by a tab, and was easy to read both programmatically and manually.
Saving the results in a TSV file gave me a reusable dataset for visualization and further analysis. It also helped avoid reprocessing the entire dataset every time I wanted to generate charts or tables. The format is supported by many data tools and spreadsheets, making the data portable and versatile.
At last, I download the ner.counts file and I perform the (git pull), (git add .) , (git commit -m" ") and (git push) so that each member could have it on their desktop.

##  3 Create a gazetteer for the NER places

Key steps:

### Reading Place Names from ner_counts.tsv:
I started with a file named ner_counts.tsv, which I had previously generated through Named Entity Recognition (NER) applied to a corpus of text—specifically, news articles. This file listed place names and the number of times each one appeared in the text. To work with this data programmatically, I used Python’s file handling capabilities to read the file line by line. I purposely skipped the first line because it was a header, and then I used .split("\t") to isolate the place name from each row. By storing just the first column of each line in a list called place_names, I effectively prepared a clean list of location names ready for geographic analysis.

### Writing and Using the get_coordinates Function:
Next, I created a custom function called get_coordinates to fetch the geographical coordinates (latitude and longitude) of each place. I used the GeoNames API, a public geographical database that can be queried using HTTP requests. I included my own registered GeoNames username (abdus.salam2) in each API call, as required. The function is designed to be reusable and flexible—it allows optional fuzzy matching (to account for minor spelling differences or incomplete names) and uses a timeout parameter to pause for a second between requests. This delay helps prevent my IP from being temporarily blocked by the API for making too many requests in a short time. Inside the function, I used the requests library to send the API request and response.json() to parse the data. If a result is returned, I extract the latitude and longitude from the first match; otherwise, I log an error and return None.

### Writing the Gazetteer File: ner_gazetteer.tsv:
With the list of place names and a working coordinate-fetching function, I began to build my own gazetteer. I created a new file called ner_gazetteer.tsv, which I opened in write mode and initialized with a header: "Name\tLatitude\tLongitude\n". Then, I looped through each place name in the place_names list. For each name, I called get_coordinates and received either a set of valid coordinates or a failure (e.g., if the place was too vague or not recognized by GeoNames). If coordinates were found, I wrote them into the file alongside the name; otherwise, I wrote "NA\tNA" to indicate missing data. This gave me a structured dataset that mapped textual place references to their geographic locations—a critical bridge between language and space.

### Displaying the Final Gazetteer:
After writing the file, I reopened it in read mode and printed its contents. This step helped me validate my output, checking for obvious errors, missing values, or formatting issues. I paid attention to entries with "NA" in the latitude and longitude fields, since those might indicate problems like spelling errors, abbreviations, or uncommon locations not covered by the GeoNames database. This step was also useful for quality assurance before moving on to mapping.

### Manually Updating Missing Coordinates:
After completing the initial automated process and generating the ner_gazetteer.tsv file, I reviewed the entries for any missing coordinates those marked as "NA" in the latitude and longitude fields. These entries typically resulted from ambiguous, misspelled, or lesser-known place names that the GeoNames API couldn't resolve. To improve the accuracy and completeness of the gazetteer, I manually searched for the coordinates of these locations from Google. I then updated the .tsv file accordingly, replacing the "NA" values with the correct latitude and longitude. This manual refinement ensured a more comprehensive and reliable dataset, which is essential for building accurate and visually coherent geographic maps.




## 4A. Map the regex-extracted placenames
This part focuses on visualizing place names extracted from the set of articles using regular expressions (regex). It gives an interactive animated map that shows the geographical distribution of these place names over time. It allows the users to explore how place name mentions change month by month.
Key steps:
###Loading Data: 
In this step two dataset were loaded: 
First is the regex counts file (regex_counts.tsv). This contains the place names and their frequencies (means how many times they appear in the articles). Second is the gazetteer file (geonames_gaza_selection.tsv). This provides us with the geographical coordinates (the latitude and longitude) for the places listed in the regex counts file. 

### Renaming Columns for Consistency: 
This ensures that the place names match between the two datsets, the ‘asciiname’ column in the gazetteer file is renamed to ‘placenmes’. This is necessary because both datasets need to have the same column name for merging process.

### Merging the Datasets: 
In this part the data from the two files is then merged into a single datsets. This is done by joining the data on the ‘placename’ column. After merging, the datasets contain the place names, their occurrence counts, and their geographical coordinates.

### Creating the Animated Map: 
Now using Plotly Express, an interactive map is generated with following information:
Latitude and Longitude: These columns position the places on the map. 
Hover information: When hovering over a place, the place name is displayed.
Point Size: The size of each point on the map relates to the count of how often that place name appears in the articles. 
Animation: The map is animated by month, so users can see how the frequency and distribution of place names change over time. 
Map Projection: The map uses the natural earth project for better and clear geographical accuracy.

### Displaying and Saving the Map: 
After the map is generated, it is displayed interactively for users to explore. Additionally, the map is saved in two formats: 
HTML: In this an interactive version of the map is saved as an HTML file. 
PNG: In this a static image of the map is saved as a PNG file. 



## 4B. Map the NER-extracted placenames:
To visualize the frequency of place names extracted using Named Entity Recognition (NER), I began by importing the required Python libraries like pandas and plotly.express. Pandas were used for data loading and manipulation, while plotly.express allowed for the creation of an interactive geographical map.
python
import pandas as pd
import plotly.express as px
Next, I loaded two datasets: ner_counts.tsv, which contains place names and their frequency of occurrence in the text, and NER_gazetteer.tsv, which contains the corresponding geographical coordinates (latitude and longitude) for those place names. Since these files are tab-separated, I specified the separator as \t:
python
ner_counts = pd.read_csv("ner_counts.tsv", sep="\t")
gazetteer = pd.read_csv("NER_gazetteer.tsv", sep="\t")

To prepare the datasets for merging, I renamed the relevant columns so both dataframes would have consistent naming conventions. This made the merge operation more reliable and reduced the chance of errors due to mismatched column names:
python
ner_counts.rename(columns={"Place": "placename", "Count": "frequency"}, inplace=True)
gazetteer.rename(columns={"Name": "placename", "Latitude": "latitude", "Longitude": "longitude"}, inplace=True)

I then merged the two dataframes using placename as the common key. This step was essential to associate each place name with its frequency and corresponding coordinates in a single unified dataframe:
python
merged = pd.merge(ner_counts, gazetteer, on="placename")


Before plotting, I added a check for missing coordinate values to ensure the map would only display valid locations. This helps prevent rendering issues or inaccuracies:
python
merged = merged.dropna(subset=["latitude", "longitude"])
For visualization, I used Plotly Express to create a scatter_geo plot. Each point on the map represents a place, and both its size and color reflect how often that place was mentioned. This makes the data easier to interpret visually, with higher-frequency places appearing more prominently:
python
fig = px.scatter_geo(
    merged,
    lat="latitude",
    lon="longitude",
    hover_name="placename",
    size="frequency",
    color="frequency",
    size_max=20,
    opacity=0.6,
    title="NER Place Name Frequencies - January 2024",
    projection="natural earth",
    color_continuous_scale="Viridis"
)
I customized the map’s appearance to improve readability by setting the land color to light gray and adjusting the margins:
python
fig.update_layout(
    geo=dict(showland=True, landcolor="LightGray"),
    margin={"r":0,"t":50,"l":0,"b":0}
)

Finally, I saved the map in two formats, a .html file for interactive use in a browser and a .png image for static inclusion in documents. I also used fig.show() to display the map during development:
python
fig.write_html("ner_map.html")
fig.write_image("ner_map.png")
fig.show()
This process provided an intuitive and visually engaging way to understand the geographical patterns in the NER data. It also helped ensure data integrity through careful preprocessing and handling of missing values.

## Describe the advantages and disadvantages of the two techniques used (regex+gazetteer and NER).
The two techniques used to extract place names are regex with a gazetteer and Named Entity Recognition (NER). Each technique has their own strengths and weaknesses. Using regex and a gazetteer can work well when we have a full list of place names and clear patterns. It is quite accurate for matching specific terms and is easy to understand and control. However, it struggles to find names that are not already in the list or that appear with different spellings. It also cannot learn patterns on its own, which means it only works within the rules we set, and that can make it a bit limited.
NER:
On the other hand, NER is more flexible. It looks at the context of words and can often pick up place names even if they are not on a list or are slightly misspelt. This means it can catch more names overall. But it is not perfect, it sometimes picks out things that are not actually places, and it is not always easy to adjust or understand why it made certain choices. Also, unless it is been trained on similar texts, like news about Gaza, its results might not always be spot-on.




## Include an image of your final maps in the README file.
![image](C:\Users\Fine Gallery\Downloads\FASDH25-portfolio2\maps)
![image](C:\Users\Fine Gallery\Downloads\FASDH25-portfolio2\maps)




## Compare the January 2024 maps generated from the regex- and NER data.
After comparing the January 2024 maps generated from the regex-based and NER-based data, a few differences stand out. The regex map identifies more locations overall, but it also includes several false positives such as ordinary words that were mistakenly tagged as places. On the other hand, the NER (Named Entity Recognition) map shows fewer places, but the ones it does pick are generally more accurate and reliable. The NER map also seems to better recognise named cities and countries, especially in conflict zones, whereas the regex map sometimes misses these or tags nearby or unrelated terms. In short, the regex approach is broader but messier, while the NER method is more precise but might miss some locations.


## Self-critical analysis: what are the weaknesses of the project as it currently is; what could be improved if there was more time?
One of the main weaknesses of our project was that we were not very familiar with coding. Since we didn’t have much experience with it, we found many parts of the project difficult. Even though we followed the steps carefully, we came across some unexpected errors that we didn’t fully understand. Also, a few parts of the instructions were not detailed enough, which made things more confusing.
If we had more time, we believe we could have done the project with better accuracy and more confidence. Extra time would have helped us fix mistakes, test our work properly, and improve our overall understanding.
Even with these challenges, the project was useful. It gave us a chance to try out different tools, use what we learned in class, and get more practice. This helped us gain more experience and feel more confident working with these types of tasks.
